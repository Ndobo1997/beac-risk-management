import streamlit as st
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import lognorm, gamma, weibull_min, norm, expon, pareto, poisson

def format_fcfa(value):
    """Format de valeurs en FCFA en millions ou milliards"""
    if abs(value) >= 1_000_000_000:
        return f"{value / 1_000_000_000:,.2f} milliards FCFA"
    elif abs(value) >= 1_000_000:
        return f"{value / 1_000_000:,.2f} million FCFA"
    else:
        return f"{value:,.0f} FCFA"

def main():
    """LDA Professionnel - M√©thodologie OpRisk avec Weibull"""
    
    st.title("üè¶ LDA Professionnel - M√©thodologie OpRisk")
    st.write("*Loss Distribution Approach - Mod√®le Weibull Optimis√©*")
    
    # Chemin du fichier Excel mis √† jour pour le dossier 'data'
    DATA_FILE_PATH = "data/base_incidents.xlsx"
    
    # Chargement donn√©es
    @st.cache_data
    def charger_donnees():
        try:
            df = pd.read_excel(DATA_FILE_PATH, sheet_name="Incidents_DOF_augmente")
            cout = pd.to_numeric(df['Cout_total_estime(en 10 000 FCFA)'], errors='coerce')
            entites = df['Entit√©'].values
            mask = (~pd.isna(cout)) & (cout > 0) & (~pd.isna(entites))
            
            return {
                'severites': cout[mask].values * 10000,  # Convert to FCFA
                'entites': entites[mask],
                'df_complet': df[mask]
            }
        except Exception as e:
            st.error(f"Erreur chargement: {e}")
            return None
    
    data = charger_donnees()
    if data is None:
        return
    
    severites = data['severites']
    entites = data['entites']
    
    st.success(f"‚úÖ **{len(severites)} observations** charg√©es (32 ann√©es)")
    
    # Configuration
    entites_uniques = pd.Series(entites).value_counts()
    st.info(f"üìä **{len(entites_uniques)} entit√©s diff√©rentes** identifi√©es")
    
    seuil_80 = np.percentile(severites, 80)
    data_filtre = severites[severites <= seuil_80]
    entites_filtre = entites[severites <= seuil_80]
    
    st.info(f"üìä **Donn√©es filtr√©es:** {len(data_filtre)} observations (80% inf√©rieurs)")
    
    # =================== √âTAPE 1: DISTRIBUTION DE S√âV√âRIT√â ===================
    
    st.header("1Ô∏è‚É£ Estimation Distribution de S√©v√©rit√©")
    
    def tester_distributions_weibull_optimise(data):
        """Tests optimis√©s pour s√©lection Weibull"""
        
        distributions = [
            ('Log-Normale', lognorm), 
            ('Weibull', weibull_min),
            ('Gamma', gamma),
            ('Pareto', pareto),
            ('Exponentielle', expon),
            ('Normale', norm)
        ]
        
        resultats = []
        
        for nom, dist in distributions:
            try:
                # Estimation MLE
                if nom == 'Log-Normale':
                    data_pos = data[data > 0]
                    log_data = np.log(data_pos)
                    mu, sigma = log_data.mean(), log_data.std()
                    params = (sigma, 0, np.exp(mu))
                    loglik = np.sum(lognorm.logpdf(data_pos, s=sigma, scale=np.exp(mu)))
                    
                elif nom in ['Normale', 'Exponentielle']:
                    params = dist.fit(data, floc=0) if nom == 'Exponentielle' else dist.fit(data)
                    loglik = np.sum(dist.logpdf(data, *params))
                
                else:
                    params = dist.fit(data, floc=0)
                    loglik = np.sum(dist.logpdf(data, *params))
                
                # Crit√®res d'information
                k = len(params)
                aic = -2 * loglik + 2 * k
                bic = -2 * loglik + k * np.log(len(data))
                
                resultats.append({
                    'Distribution': nom,
                    'AIC': aic,
                    'BIC': bic,
                    'Params': params
                })
                
            except:
                continue
        
        return pd.DataFrame(resultats)
    
    # Tests et s√©lection
    df_sev = tester_distributions_weibull_optimise(data_filtre)
    
    if len(df_sev) > 0:
        df_sev = df_sev.sort_values('AIC')
        
        st.subheader("üìä Comparaison des Distributions")
        
        # Affichage simplifi√©
        df_display = df_sev[['Distribution', 'AIC']].round(3)
        
        st.dataframe(df_display)
        
        # S√©lection Weibull forc√©e
        weibull_row = df_sev[df_sev['Distribution'] == 'Weibull']
        
        if len(weibull_row) > 0:
            best_sev = weibull_row.iloc[0]
            st.success(f"üèÜ **Distribution S√©lectionn√©e : WEIBULL**")
            
            # Param√®tres Weibull
            c, _, scale = best_sev['Params']
            col1, col2 = st.columns(2)
            col1.metric("**Forme (c)**", f"{c:.3f}")
            col2.metric("**√âchelle**", f"{scale:,.0f} FCFA")
            
            st.info(f"‚úÖ **AIC optimal:** {best_sev['AIC']:.1f}")
            
        else:
            best_sev = df_sev.iloc[0]
            st.warning(f"‚ö†Ô∏è **Distribution Retenue:** {best_sev['Distribution']} (Weibull non disponible)")
        
        # =================== R√âSULTATS PAR ENTIT√â ===================
        
        st.subheader("üìà R√©sultats par Entit√©")
        
        resultats_entites = []
        for entite in entites_uniques.index:
            severites_entite = data_filtre[entites_filtre == entite]
            if len(severites_entite) > 5:
                resultats_entites.append({
                    'Entit√©': entite.replace('DGEFRI-DOF : ', ''),
                    'N_Incidents': len(severites_entite),
                    'S√©v√©rit√©_Moyenne': np.mean(severites_entite),
                    'VaR_95': np.percentile(severites_entite, 95)
                })
        
        if resultats_entites:
            df_entites = pd.DataFrame(resultats_entites)
            # Format columns for display
            df_entites['S√©v√©rit√©_Moyenne'] = df_entites['S√©v√©rit√©_Moyenne'].apply(format_fcfa)
            df_entites['VaR_95'] = df_entites['VaR_95'].apply(format_fcfa)
            st.dataframe(df_entites)
        
        # R√©sum√© s√©v√©rit√©
        col1, col2, col3 = st.columns(3)
        col1.metric("**S√©v√©rit√© Moyenne**", format_fcfa(np.mean(data_filtre)))
        col2.metric("**S√©v√©rit√© M√©diane**", format_fcfa(np.median(data_filtre)))
        col3.metric("**VaR 95%**", format_fcfa(np.percentile(data_filtre, 95)))
        
    else:
        st.error("‚ùå Aucune distribution ajust√©e")
        return
    
    # =================== √âTAPE 2: DISTRIBUTION DE FR√âQUENCE ===================
    
    st.header("2Ô∏è‚É£ Estimation Distribution de Fr√©quence")
    
    lambda_global = len(entites_filtre) / 32
    
    st.subheader("üìä Param√®tres de Fr√©quence")
    col1, col2 = st.columns(2)
    col1.metric("**Œª Global**", f"{lambda_global:.2f}")
    col2.metric("**Incidents/An**", f"{lambda_global:.1f}")
    
    # Fr√©quence par entit√©
    frequences_entites = []
    for entite in entites_uniques.index:
        n_incidents = np.sum(entites_filtre == entite)
        lambda_entite = n_incidents / 32
        
        frequences_entites.append({
            'Entit√©': entite.replace('DGEFRI-DOF : ', ''),
            'Lambda': lambda_entite
        })
    
    df_freq = pd.DataFrame(frequences_entites)
    st.dataframe(df_freq.round(3))
    
    # =================== √âTAPE 3: CAPITAL √âCONOMIQUE ===================
    
    st.header("3Ô∏è‚É£ Estimation du Capital √âconomique")
    
    def calculer_capital_weibull(params, lambda_freq, n_sim=50000):
        """Calcul capital avec Weibull"""
        pertes_annuelles = []
        
        for _ in range(n_sim):
            N = np.random.poisson(lambda_freq)
            perte_totale = 0
            
            if N > 0:
                if best_sev['Distribution'] == 'Weibull':
                    severites = weibull_min.rvs(*params, size=N)
                elif best_sev['Distribution'] == 'Log-Normale':
                    severites = lognorm.rvs(*params, size=N)
                elif best_sev['Distribution'] == 'Gamma':
                    severites = gamma.rvs(*params, size=N)
                else:
                    severites = norm.rvs(*params, size=N)
                severites = severites[severites > 0]
                
                perte_totale = np.sum(severites)
            
            pertes_annuelles.append(perte_totale)
        
        pertes = np.array(pertes_annuelles)
        return {
            'var_95': np.percentile(pertes, 95),  # In FCFA
            'var_99': np.percentile(pertes, 99),  # In FCFA
            'var_999': np.percentile(pertes, 99.9),  # In FCFA
            'es_999': np.mean(pertes[pertes >= np.percentile(pertes, 99.9)])  # In FCFA
        }
    
    # Simulation
    with st.spinner("üé≤ Simulation Monte Carlo..."):
        capital = calculer_capital_weibull(best_sev['Params'], lambda_global)
    
    # R√©sultats capital
    col1, col2, col3 = st.columns(3)
    col1.metric("**VaR 95%**", format_fcfa(capital['var_95']))
    col2.metric("**VaR 99%**", format_fcfa(capital['var_99']))
    col3.metric("**VaR 99.9%**", format_fcfa(capital['var_999']))
    
    capital_fcfa = capital['var_999']  # Already in FCFA
    st.metric("**üè¶ CAPITAL R√âGLEMENTAIRE**", format_fcfa(capital_fcfa))
    
    # Capital par entit√©
    capital_entites = []
    for _, row in df_freq.iterrows():
        contribution = row['Lambda'] / lambda_global
        capital_entites.append({
            'Entit√©': row['Entit√©'],
            'Contribution_%': contribution * 100,
            'Capital_FCFA': capital['var_999'] * contribution
        })
    
    df_capital = pd.DataFrame(capital_entites)
    df_capital['Capital_FCFA'] = df_capital['Capital_FCFA'].apply(format_fcfa)
    df_capital['Contribution_%'] = df_capital['Contribution_%'].round(2)
    st.dataframe(df_capital)
    
    # =================== SYNTH√àSE ===================
    
    st.header("üìã Synth√®se LDA")
    
    st.success(f"""
**üéØ R√âSULTATS FINAUX - MOD√àLE WEIBULL**

**üìä S√©v√©rit√©:** {best_sev['Distribution']}
- Param√®tres: c={best_sev['Params'][0]:.3f}, √©chelle={format_fcfa(best_sev['Params'][2])}

**üìà Fr√©quence:** Poisson Œª={lambda_global:.2f}
**üí∞ Capital Total:** {format_fcfa(capital_fcfa)}
**üìä VaR 99.9%:** {format_fcfa(capital['var_999'])}
**üìà Expected Shortfall:** {format_fcfa(capital['es_999'])}
""")
    
    # Bouton pour revenir √† l'accueil
    if st.button("‚Üê Retour √† l'Accueil"):
        st.session_state.current_page = 'accueil'
        st.session_state.lda_subpage = None
        if 'redirect_to' in st.session_state:
            del st.session_state.redirect_to
        st.rerun()
    
    return {
        'distribution': best_sev['Distribution'],
        'capital': capital_fcfa,
        'var_999': capital['var_999']
    }

if __name__ == "__main__":
    st.set_page_config(page_title="LDA Weibull", page_icon="üè¶", layout="wide")
    
    if st.button("üöÄ **ANALYSER AVEC LDA WEIBULL**"):
        resultats = main()
        if resultats:
            st.balloons()
            st.success("‚úÖ **Analyse LDA Weibull termin√©e !**")